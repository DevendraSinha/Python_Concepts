{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOe2GsWpkaRV8agUIUPIJyT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##                                               ** BASIC VECTORIZATION TECHNIQUES**"],"metadata":{"id":"4C2DGmExCltR"}},{"cell_type":"markdown","source":["## **1.Bag of Words**"],"metadata":{"id":"1H1YNeEnzo7V"}},{"cell_type":"markdown","source":["**Advantages of Count Vectorizer**\n","\n","Simple and straightforward implementation.\n","Effective for tasks where word frequency is a key feature.\n","\n","**Disadvantages of Count Vectorizer**\n","\n","Similar to BoW, it produces high-dimensional and sparse matrices.\n","Ignores the context and order of words.\n","Limited ability to capture semantic meaning"],"metadata":{"id":"NdDHmF5M56XR"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sBHV30izX6q","executionInfo":{"status":"ok","timestamp":1740802046906,"user_tz":-330,"elapsed":58,"user":{"displayName":"Devendra Sinha","userId":"01100308281361717778"}},"outputId":"472f115d-07df-4b52-e0fc-126acf20e9d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n"]},{"output_type":"execute_result","data":{"text/plain":["array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], dtype=object)"]},"metadata":{},"execution_count":6}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample corpus of text documents\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","\n","# Create a CountVectorizer object\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the corpus into a Bag-of-Words representation\n","X = vectorizer.fit_transform(corpus)\n","\n","# Get the feature names (vocabulary)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Print the Bag-of-Words matrix\n","print(X.toarray())\n","\n","# Print the feature names\n","feature_names"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","#corpus = ['Text processing is necessary.', 'Text processing is necessary and important.', 'Text processing is easy.']\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrtKdJakBpkK","executionInfo":{"status":"ok","timestamp":1740804460172,"user_tz":-330,"elapsed":23,"user":{"displayName":"Devendra Sinha","userId":"01100308281361717778"}},"outputId":"8cf64b9d-137c-4964-f534-46d91fde57d8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n","[[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n"]}]},{"cell_type":"code","source":["! pip install BagOfWords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bFLOj8yK4rtx","executionInfo":{"status":"ok","timestamp":1740802027974,"user_tz":-330,"elapsed":2628,"user":{"displayName":"Devendra Sinha","userId":"01100308281361717778"}},"outputId":"a90a5132-04b9-46f9-b4c5-642a8ca270a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting BagOfWords\n","  Downloading bagofwords-1.0.4.tar.gz (14 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Discarding \u001b[4;34mhttps://files.pythonhosted.org/packages/02/40/b2c96601b3a205a3ad511fd3342bbf0f23608a67348c6dcd09e9b44088b9/bagofwords-1.0.4.tar.gz (from https://pypi.org/simple/bagofwords/)\u001b[0m: \u001b[33mRequested BagOfWords from https://files.pythonhosted.org/packages/02/40/b2c96601b3a205a3ad511fd3342bbf0f23608a67348c6dcd09e9b44088b9/bagofwords-1.0.4.tar.gz has inconsistent version: expected '1.0.4', but metadata has '1.0.3'\u001b[0m\n","  Downloading bagofwords-1.0.1.tar.gz (11 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"markdown","source":["### **2.Count Vectorizer**"],"metadata":{"id":"50WjfJ7x6Q9V"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample documents\n","documents = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log mat.\",\n","    \"Cats and dogs are pets.\"\n","]\n","\n","# Initialize CountVectorizer\n","count_vectorizer = CountVectorizer()\n","X_count = count_vectorizer.fit_transform(documents)\n","#feature_names = vectorizer.get_feature_names_out()\n","# Convert to array and print\n","print(X_count.toarray())\n","print(count_vectorizer.get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcK_JTIp4XTa","executionInfo":{"status":"ok","timestamp":1740804504111,"user_tz":-330,"elapsed":56,"user":{"displayName":"Devendra Sinha","userId":"01100308281361717778"}},"outputId":"980d2e72-40fb-4622-8b68-4a3fe570da7c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 0 0 0 1 1 0 1 2]\n"," [0 0 0 0 1 0 1 1 1 0 1 2]\n"," [1 1 0 1 0 1 0 0 0 1 0 0]]\n","['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'log' 'mat' 'on' 'pets' 'sat' 'the']\n"]}]},{"cell_type":"markdown","source":["### **3.TF-IDF**"],"metadata":{"id":"PhNbykO66fE5"}},{"cell_type":"markdown","source":["Term Frequency (TF): Measures the frequency of a word in a document.\n","\n","TF =Number of times term t appears in document d/Total number of terms in document\n","d\n","TF(t,d)=\n","Total number of terms in document d\n","Number of times term t appears in document d\n","​\n","\n","\n"],"metadata":{"id":"AR0OvXUs_gZG"}},{"cell_type":"markdown","source":["Disadvantages of TF-IDF\n","\n","Still results in sparse matrices.\n","Does not capture word order or context.\n","Computationally more expensive than BoW."],"metadata":{"id":"80-sH2x6_9Si"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Sample documents\n","documents = [\n","    \"The cat sat on the mat.\",\n","    \"The dog sat on the log.\",\n","    \"Cats and dogs are pets.\"\n","]\n","\n","# Initialize TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Fit and transform the documents\n","tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n","\n","# Get feature names (vocabulary)\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Print the TF-IDF matrix\n","print(tfidf_matrix.toarray())\n","\n","# Print the feature names\n","feature_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqm92jCS-REZ","executionInfo":{"status":"ok","timestamp":1740803480974,"user_tz":-330,"elapsed":23,"user":{"displayName":"Devendra Sinha","userId":"01100308281361717778"}},"outputId":"a52f1a60-bc81-41ef-da7d-075818db9506"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.         0.42755362 0.         0.         0.\n","  0.         0.42755362 0.32516555 0.         0.32516555 0.6503311 ]\n"," [0.         0.         0.         0.         0.42755362 0.\n","  0.42755362 0.         0.32516555 0.         0.32516555 0.6503311 ]\n"," [0.4472136  0.4472136  0.         0.4472136  0.         0.4472136\n","  0.         0.         0.         0.4472136  0.         0.        ]]\n"]},{"output_type":"execute_result","data":{"text/plain":["array(['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'log', 'mat', 'on',\n","       'pets', 'sat', 'the'], dtype=object)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### **                                           ADVANCED VECTORIZATION TECHNIQUES**"],"metadata":{"id":"iJsLgJFzC6ba"}},{"cell_type":"markdown","source":["# **1.Word Embeddings**"],"metadata":{"id":"SsDeZsvwDH_0"}}]}